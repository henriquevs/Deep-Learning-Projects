# Week 2 Exercises

Week 2 — optimization techniques such as mini-batch gradient descent, (Stochastic) gradient descent, Momentum, RMSProp, Adam and learning rate decay etc.