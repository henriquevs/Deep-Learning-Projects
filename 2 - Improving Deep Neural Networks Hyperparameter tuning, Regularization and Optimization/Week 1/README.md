# Week 1 Exercises

Week 1 — how to initialize complex neural networks, the difference between train/dev/test sets, bias and variance issues in network models, dropout or L2 regularization techniques, and vanishing / exploding gradients etc. We also learned a technique called gradient checking which helps debug the implementation of back prorogation.

Exercises completed during the second week of the course:

* Network initialization.
* Network regularization.
* Gradient checking.